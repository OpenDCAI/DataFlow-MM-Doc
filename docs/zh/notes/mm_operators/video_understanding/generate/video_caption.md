---
title: è§†é¢‘æè¿°ç”Ÿæˆ
createTime: 2025/07/16 14:50:59
icon: material-symbols-light:interpreter-mode
permalink: /zh/mm_operators/video_understanding/generate/video_caption/
---

## ğŸ“˜ æ¦‚è¿°

`VideoToCaptionGenerator` æ˜¯ä¸€ä¸ªç”¨äº**è°ƒç”¨è§†è§‰è¯­è¨€å¤§æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æè¿°ï¼ˆVideo Captionï¼‰**çš„ç®—å­ã€‚  
å®ƒä¼šæ ¹æ®è¾“å…¥è§†é¢‘ï¼Œè‡ªåŠ¨æ„å»ºæç¤ºè¯ï¼Œå¼•å¯¼æ¨¡å‹è¾“å‡ºé«˜è´¨é‡çš„è§†é¢‘å†…å®¹æè¿°ï¼Œé€‚ç”¨äºè§†é¢‘æ ‡æ³¨ã€å¤šæ¨¡æ€æ•°æ®é›†æ„å»ºã€è§†é¢‘ç†è§£ç­‰åœºæ™¯ã€‚

---

## ğŸ—ï¸ `__init__` å‡½æ•°

```python
def __init__(
    self,
    vlm_serving: VLMServingABC,
    prompt_template: Optional[VideoCaptionGeneratorPrompt | DiyVideoPrompt | str] = None
):
    ...
```

## ğŸ§¾ `__init__` å‚æ•°è¯´æ˜

| å‚æ•°å              | ç±»å‹                                                              | é»˜è®¤å€¼    | è¯´æ˜                                |
| :--------------- | :-------------------------------------------------------------- | :----- | :-------------------------------- |
| `vlm_serving`    | `VLMServingABC`                                                 | -      | VLMæ¨¡å‹æœåŠ¡å¯¹è±¡ï¼Œç”¨äºè°ƒç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆè§†é¢‘æè¿°       |
| `prompt_template` | `VideoCaptionGeneratorPrompt` \| `DiyVideoPrompt` \| `str` \| `None` | `None` | Promptæ¨¡æ¿ï¼Œé»˜è®¤ä¸º"Please describe the video in detail." |

---

## âš¡ `run` å‡½æ•°

```python
def run(
    self,
    storage: DataFlowStorage,
    input_image_key: str = "image",
    input_video_key: str = "video",
    input_audio_key: str = "audio",
    input_conversation_key: str = "conversation",
    output_key: str = "caption"
):
    ...
```

`run` æ˜¯ç®—å­ä¸»é€»è¾‘ï¼Œæ‰§è¡Œè§†é¢‘æè¿°ç”Ÿæˆä»»åŠ¡ï¼š
è¯»å–è§†é¢‘è·¯å¾„ â†’ æ„å»ºæç¤ºè¯ â†’ è°ƒç”¨VLMæ¨¡å‹ â†’ ç”Ÿæˆæ–‡æœ¬æè¿° â†’ å†™å…¥è¾“å‡ºæ–‡ä»¶ã€‚

## ğŸ§¾ `run` å‚æ•°è¯´æ˜

| å‚æ•°å                      | ç±»å‹                | é»˜è®¤å€¼              | è¯´æ˜              |
| :----------------------- | :---------------- | :--------------- | :-------------- |
| `storage`                | `DataFlowStorage` | -                | Dataflow æ•°æ®å­˜å‚¨å¯¹è±¡ |
| `input_image_key`        | `str`             | `"image"`        | è¾“å…¥æ•°æ®ä¸­å›¾åƒå­—æ®µå      |
| `input_video_key`        | `str`             | `"video"`        | è¾“å…¥æ•°æ®ä¸­è§†é¢‘å­—æ®µå      |
| `input_audio_key`        | `str`             | `"audio"`        | è¾“å…¥æ•°æ®ä¸­éŸ³é¢‘å­—æ®µå      |
| `input_conversation_key` | `str`             | `"conversation"` | è¾“å…¥æ•°æ®ä¸­å¯¹è¯å­—æ®µå      |
| `output_key`             | `str`             | `"caption"`      | æ¨¡å‹è¾“å‡ºå­—æ®µå         |

---

## ğŸ§  ç¤ºä¾‹ç”¨æ³•

```python
from dataflow.operators.core_vision import VideoToCaptionGenerator
from dataflow.serving import LocalModelVLMServing_vllm
from dataflow.utils.storage import FileStorage

# Step 1: å¯åŠ¨æœ¬åœ°æ¨¡å‹æœåŠ¡
vlm_serving = LocalModelVLMServing_vllm(
    hf_model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
    hf_cache_dir="./model_cache",
    vllm_tensor_parallel_size=1,
    vllm_temperature=0.7,
    vllm_top_p=0.9,
    vllm_max_tokens=2048,
    vllm_max_model_len=51200,
    vllm_gpu_memory_utilization=0.9
)

# Step 2: å‡†å¤‡è¾“å…¥æ•°æ®
storage = FileStorage(
    first_entry_file_name="./sample_data.json",
    cache_path="./cache",
    file_name_prefix="video_caption",
    cache_type="json",
)
storage.step()

# Step 3: åˆå§‹åŒ–å¹¶è¿è¡Œç®—å­
video_caption_generator = VideoToCaptionGenerator(
    vlm_serving=vlm_serving,
)
video_caption_generator.run(
    storage=storage,
    input_video_key="video",
    input_conversation_key="conversation",
    output_key="caption"
)
```

---

## ğŸ§¾ é»˜è®¤è¾“å‡ºæ ¼å¼ï¼ˆOutput Formatï¼‰

| å­—æ®µ             | ç±»å‹           | è¯´æ˜          |
| :------------- | :----------- | :---------- |
| `video`        | `List[str]`  | è¾“å…¥è§†é¢‘è·¯å¾„      |
| `conversation` | `List[Dict]` | å¯¹è¯å†å²        |
| `caption`      | `str`        | æ¨¡å‹ç”Ÿæˆçš„è§†é¢‘æè¿°æ–‡æœ¬ |

---

### ğŸ“¥ ç¤ºä¾‹è¾“å…¥

```json
{"video": ["./test/example_video.mp4"], "conversation": [{"from": "human", "value": ""}]}
```

### ğŸ“¤ ç¤ºä¾‹è¾“å‡º

```json
{
  "video": ["./test/example_video.mp4"],
  "conversation": [{"from": "human", "value": "Please describe the video in detail."}],
  "caption": "This video shows a person walking in a park on a sunny day. The person is wearing casual clothes and appears to be enjoying the outdoor scenery."
}
```

---

## ğŸ¯ è‡ªå®šä¹‰ Prompt

é»˜è®¤promptä¸ºï¼š"Please describe the video in detail."

å¦‚éœ€è‡ªå®šä¹‰ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ï¼š

### æ–¹å¼1ï¼šä½¿ç”¨å­—ç¬¦ä¸²

```python
video_caption_generator = VideoToCaptionGenerator(
    vlm_serving=vlm_serving,
    prompt_template="è¯·è¯¦ç»†æè¿°è¿™ä¸ªè§†é¢‘çš„å†…å®¹ã€åœºæ™¯å’Œä¸»è¦æ´»åŠ¨ã€‚"
)
```

### æ–¹å¼2ï¼šä½¿ç”¨è‡ªå®šä¹‰Promptç±»

```python
from dataflow.prompts.video import DiyVideoPrompt

custom_prompt = DiyVideoPrompt(
    "Describe the video focusing on: {aspect}"
)

video_caption_generator = VideoToCaptionGenerator(
    vlm_serving=vlm_serving,
    prompt_template=custom_prompt
)
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. âœ… è¾“å…¥æ•°æ®å¿…é¡»åŒ…å« `conversation` å­—æ®µï¼Œæ ¼å¼ä¸ºå¯¹è¯åˆ—è¡¨
2. âœ… è§†é¢‘æ–‡ä»¶è·¯å¾„éœ€è¦æ˜¯å¯è®¿é—®çš„æœ¬åœ°è·¯å¾„æˆ–URL
3. âœ… VLMæ¨¡å‹éœ€è¦æ”¯æŒè§†é¢‘è¾“å…¥ï¼ˆå¦‚Qwen2.5-VLç³»åˆ—ï¼‰
4. âœ… æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´ `vllm_gpu_memory_utilization` å’Œ `vllm_tensor_parallel_size`

---

## ğŸ”— ç›¸å…³é“¾æ¥

- **ä»£ç :** [VideoToCaptionGenerator](https://github.com/OpenDCAI/DataFlow-MM/blob/main/dataflow/operators/core_vision/generate/video_caption_generator.py)
- **æµ‹è¯•è„šæœ¬:** [test_video_caption.py](https://github.com/OpenDCAI/DataFlow-MM/blob/main/test/test_video_caption.py)
