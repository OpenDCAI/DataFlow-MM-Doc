---
title: è§†é¢‘é—®ç­”ç”Ÿæˆï¼ˆVideoCaptionToQAGeneratorï¼‰
createTime: 2025/12/20 11:30:00
permalink: /zh/mm_operators/video_understanding/generate/video_qa/
---

## ğŸ“˜ æ¦‚è¿°

`VideoCaptionToQAGenerator` æ˜¯ä¸€ä¸ªç”¨äº **åŸºäºè§†é¢‘å­—å¹•è‡ªåŠ¨ç”Ÿæˆé—®ç­”å¯¹ï¼ˆVideo QAï¼‰** çš„ç®—å­ã€‚  
å®ƒä¼šæ ¹æ®è¾“å…¥çš„è§†é¢‘å­—å¹•ï¼ˆcaptionï¼‰ï¼Œè‡ªåŠ¨æ„å»ºæç¤ºè¯ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆä¸è§†é¢‘å†…å®¹ç›¸å…³çš„é—®é¢˜å’Œç­”æ¡ˆï¼Œé€‚ç”¨äºè§†é¢‘é—®ç­”æ•°æ®é›†æ„å»ºã€è§†é¢‘ç†è§£è¯„æµ‹ã€å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿç­‰åœºæ™¯ã€‚

---

## ğŸ—ï¸ `__init__` å‡½æ•°

```python
def __init__(
    self,
    vlm_serving: VLMServingABC,
    prompt_template: Optional[VideoQAGeneratorPrompt | DiyVideoPrompt | str] = None,
    use_video_input: bool = True,
):
    ...
```

## ğŸ§¾ `__init__` å‚æ•°è¯´æ˜

| å‚æ•°å              | ç±»å‹                                                              | é»˜è®¤å€¼    | è¯´æ˜                                            |
| :--------------- | :-------------------------------------------------------------- | :----- | :-------------------------------------------- |
| `vlm_serving`    | `VLMServingABC`                                                 | -      | VLMæ¨¡å‹æœåŠ¡å¯¹è±¡ï¼Œç”¨äºè°ƒç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆé—®ç­”                      |
| `prompt_template` | `VideoQAGeneratorPrompt` \| `DiyVideoPrompt` \| `str` \| `None` | `None` | Promptæ¨¡æ¿ï¼Œé»˜è®¤ä½¿ç”¨ `VideoQAGeneratorPrompt`       |
| `use_video_input` | `bool`                                                          | `True` | æ˜¯å¦ä½¿ç”¨è§†é¢‘ä½œä¸ºè¾“å…¥ï¼ˆFalseæ—¶ä»…ä½¿ç”¨å­—å¹•æ–‡æœ¬ï¼Œä¸è¾“å…¥è§†é¢‘åˆ°æ¨¡å‹ï¼Œé€‚ç”¨äºçº¯æ–‡æœ¬QAï¼‰ |

---

## âš¡ `run` å‡½æ•°

```python
def run(
    self,
    storage: DataFlowStorage,
    input_image_key: str = None,
    input_video_key: str = None,
    input_conversation_key: str = "conversation",
    output_key: str = "answer",
):
    ...
```

`run` æ˜¯ç®—å­ä¸»é€»è¾‘ï¼Œæ‰§è¡Œè§†é¢‘é—®ç­”ç”Ÿæˆä»»åŠ¡ï¼š
è¯»å–å­—å¹•æ–‡æœ¬ â†’ æ„å»ºQAç”Ÿæˆæç¤ºè¯ â†’ è°ƒç”¨VLMæ¨¡å‹ â†’ ç”Ÿæˆé—®ç­”å¯¹ â†’ å†™å…¥è¾“å‡ºæ–‡ä»¶ã€‚

## ğŸ§¾ `run` å‚æ•°è¯´æ˜

| å‚æ•°å                      | ç±»å‹                | é»˜è®¤å€¼              | è¯´æ˜                              |
| :----------------------- | :---------------- | :--------------- | :------------------------------ |
| `storage`                | `DataFlowStorage` | -                | Dataflow æ•°æ®å­˜å‚¨å¯¹è±¡                 |
| `input_image_key`        | `str`             | `None`           | è¾“å…¥æ•°æ®ä¸­å›¾åƒå­—æ®µåï¼ˆå¯é€‰ï¼‰                  |
| `input_video_key`        | `str`             | `None`           | è¾“å…¥æ•°æ®ä¸­è§†é¢‘å­—æ®µåï¼ˆå¯é€‰ï¼‰                  |
| `input_conversation_key` | `str`             | `"conversation"` | è¾“å…¥æ•°æ®ä¸­å¯¹è¯å­—æ®µå                      |
| `output_key`             | `str`             | `"answer"`       | æ¨¡å‹ç”Ÿæˆçš„é—®ç­”ç»“æœå­—æ®µå                    |

---

## ğŸ§  ç¤ºä¾‹ç”¨æ³•

```python
from dataflow.operators.core_vision import VideoCaptionToQAGenerator
from dataflow.serving import LocalModelVLMServing_vllm
from dataflow.utils.storage import FileStorage

# Step 1: å¯åŠ¨æœ¬åœ°æ¨¡å‹æœåŠ¡
vlm_serving = LocalModelVLMServing_vllm(
    hf_model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
    hf_cache_dir="./model_cache",
    vllm_tensor_parallel_size=1,
    vllm_temperature=0.7,
    vllm_top_p=0.9,
    vllm_max_tokens=2048,
    vllm_max_model_len=51200,
    vllm_gpu_memory_utilization=0.9
)

# Step 2: å‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆå¿…é¡»åŒ…å«captionå­—æ®µï¼‰
storage = FileStorage(
    first_entry_file_name="./video_captions.json",
    cache_path="./cache",
    file_name_prefix="video_qa",
    cache_type="json",
)
storage.step()

# Step 3: åˆå§‹åŒ–å¹¶è¿è¡Œç®—å­
qa_generator = VideoCaptionToQAGenerator(
    vlm_serving=vlm_serving,
    use_video_input=True,  # ä½¿ç”¨è§†é¢‘è¾“å…¥
)
qa_generator.run(
    storage=storage,
    input_video_key="video",
    input_conversation_key="conversation",
    output_key="answer"
)
```

---

## ğŸ§¾ è¾“å…¥æ ¼å¼è¦æ±‚ï¼ˆInput Formatï¼‰

| å­—æ®µ             | ç±»å‹           | è¯´æ˜                    |
| :------------- | :----------- | :-------------------- |
| `caption`      | `str`        | è§†é¢‘å­—å¹•æ–‡æœ¬ï¼ˆå¿…éœ€ï¼‰            |
| `video`        | `List[str]`  | è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå½“ä½¿ç”¨è§†é¢‘è¾“å…¥æ—¶ï¼‰    |
| `image`        | `List[str]`  | å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰          |
| `conversation` | `List[Dict]` | å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼Œä¼šè¢«è‡ªåŠ¨åˆ›å»ºæˆ–æ›´æ–°ï¼‰    |

---

### ğŸ“¥ ç¤ºä¾‹è¾“å…¥

```json
{
  "caption": "A person is walking in a park on a sunny day. They are wearing casual clothes and appear to be enjoying the outdoor scenery.",
  "video": ["./test/example_video.mp4"],
  "conversation": [{"from": "human", "value": ""}]
}
```

### ğŸ“¤ ç¤ºä¾‹è¾“å‡º

```json
{
  "caption": "A person is walking in a park on a sunny day. They are wearing casual clothes and appear to be enjoying the outdoor scenery.",
  "video": ["./test/example_video.mp4"],
  "conversation": [
    {
      "from": "human",
      "value": "Based on this caption: 'A person is walking in a park on a sunny day. They are wearing casual clothes and appear to be enjoying the outdoor scenery.', please generate relevant questions and answers about the video."
    }
  ],
  "answer": "Q1: What is the person doing in the video?\nA1: The person is walking in a park.\n\nQ2: What is the weather like in the video?\nA2: It is a sunny day.\n\nQ3: What is the person wearing?\nA3: The person is wearing casual clothes."
}
```

---

## ğŸ¨ è‡ªå®šä¹‰ Prompt

é»˜è®¤ prompt æ ¼å¼ä¸ºï¼š
```
Based on this caption: '{caption}', please generate relevant questions and answers about the video.
```

### æ–¹å¼1ï¼šä½¿ç”¨å­—ç¬¦ä¸²

```python
qa_generator = VideoCaptionToQAGenerator(
    vlm_serving=vlm_serving,
    prompt_template="æ ¹æ®ä»¥ä¸‹å­—å¹•å†…å®¹ï¼š'{caption}'ï¼Œè¯·ç”Ÿæˆ3ä¸ªä¸è§†é¢‘ç›¸å…³çš„é—®ç­”å¯¹ã€‚"
)
```

### æ–¹å¼2ï¼šä½¿ç”¨è‡ªå®šä¹‰Promptç±»

```python
from dataflow.prompts.video import DiyVideoPrompt

custom_prompt = DiyVideoPrompt(
    "Caption: {caption}\n\nGenerate 5 QA pairs in the format:\nQ: ...\nA: ..."
)

qa_generator = VideoCaptionToQAGenerator(
    vlm_serving=vlm_serving,
    prompt_template=custom_prompt
)
```

---

## ğŸ”„ å…¸å‹å·¥ä½œæµ

```python
from dataflow.operators.core_vision import (
    VideoToCaptionGenerator,     # Step 1: ç”Ÿæˆè§†é¢‘å­—å¹•
    VideoCaptionToQAGenerator    # Step 2: åŸºäºå­—å¹•ç”ŸæˆQA
)

# Step 1: ä¸ºè§†é¢‘ç”Ÿæˆå­—å¹•
caption_generator = VideoToCaptionGenerator(vlm_serving=vlm_serving)
caption_generator.run(storage.step())

# Step 2: åŸºäºå­—å¹•ç”ŸæˆQA
qa_generator = VideoCaptionToQAGenerator(
    vlm_serving=vlm_serving,
    use_video_input=True,  # True: ä½¿ç”¨è§†é¢‘å’Œå­—å¹•ï¼›False: ä»…ä½¿ç”¨å­—å¹•
)
qa_generator.run(storage.step())
```

---

## ğŸ§¾ é»˜è®¤è¾“å‡ºæ ¼å¼ï¼ˆOutput Formatï¼‰

| å­—æ®µ             | ç±»å‹           | è¯´æ˜           |
| :------------- | :----------- | :----------- |
| `caption`      | `str`        | è¾“å…¥çš„è§†é¢‘å­—å¹•      |
| `video`        | `List[str]`  | è§†é¢‘æ–‡ä»¶è·¯å¾„       |
| `conversation` | `List[Dict]` | æ›´æ–°åçš„å¯¹è¯å†å²     |
| `answer`       | `str`        | æ¨¡å‹ç”Ÿæˆçš„é—®ç­”å¯¹æ–‡æœ¬   |

---

## ğŸ”— ç›¸å…³é“¾æ¥

- **ä»£ç :** [VideoCaptionToQAGenerator](https://github.com/OpenDCAI/DataFlow-MM/blob/main/dataflow/operators/core_vision/generate/video_qa_generator.py)
- **ç›¸å…³ç®—å­:**
  - [VideoToCaptionGenerator](./video_caption.md) - è§†é¢‘æè¿°ç”Ÿæˆ
  - [VideoMergedCaptionGenerator](./video_merged_caption.md) - è§†é¢‘åˆå¹¶å­—å¹•ç”Ÿæˆ
  - [VideoCOTQAGenerator](./video_cotqa.md) - è§†é¢‘é“¾å¼æ€è€ƒé—®ç­”ç”Ÿæˆ

