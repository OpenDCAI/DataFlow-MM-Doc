---
title: å¤šè§’è‰²è§†é¢‘é—®ç­”ç”Ÿæˆ(MultiRole Video QA Generation)
createTime: 2025/12/2 20:00:00
icon: material-symbols-light:video
permalink: /zh/mm_operators/generate/multirole_videoqa/
---

## ğŸ“˜ æ¦‚è¿°

`MultiroleVideoQAGenerate` æ˜¯ä¸€ä¸ªæ•°æ®ç”Ÿæˆç®—å­ï¼Œç”¨äº**åŸºäºé¢„å¤„ç†è§†é¢‘æ•°æ®è‡ªåŠ¨åˆ›å»ºé—®ç­”å¯¹ï¼ˆQA Pairsï¼‰**ã€‚ 
ç»™å®šè¾“å…¥çš„é¢„å¤„ç†è§†é¢‘æ•°æ®ï¼Œå®ƒä¼šæ„å»ºå¤šä¸ªä¸è¯¥è§†é¢‘ç›¸å…³çš„é—®ç­”å¯¹ã€‚è¯¥ç®—å­é€‚ç”¨äº**å¹¿å‘Šè§†é¢‘æ ‡æ³¨**ã€**æ•°æ®é›†æ„å»º**å’Œ**è§†é¢‘ç†è§£**ä»»åŠ¡ã€‚

**åŠŸèƒ½ç‰¹æ€§ï¼š**
* æ”¯æŒ**æ‰¹é‡å¤„ç†**å¤šä¸ªé¢„å¤„ç†è§†é¢‘æ•°æ®ã€‚
* ä½¿ç”¨ **VLMï¼ˆå¦‚ Qwen2.5-VLï¼‰**ç”Ÿæˆ**é«˜è´¨é‡**çš„é—®ç­”å¯¹ã€‚
* è‡ªåŠ¨å¤„ç†è§†é¢‘è¾“å…¥å¹¶ä½¿ç”¨ Prompt ç”Ÿæˆæ•°æ®ã€‚

---

## ğŸ—ï¸ `__init__` å‡½æ•°

```python
def __init__(
    self,
    llm_serving: VLMServingABC
):
    ...
```
## ğŸ§¾ `__init__` å‚æ•°

| Parameter     | Type            | Default | Description                                                     |
| :------------ | :-------------- | :------ | :-------------------------------------------------------------- |
| `llm_serving` | `VLMServingABC` | -       | **Model Serving Object** used to call the VLM for QA pairs generation |

-----

## âš¡ `run` å‡½æ•°

```python
def run(
        self,
        storage: DataFlowStorage,
        input_meta_key: str = "Meta", 
        input_clips_key: str = "Clips", 
        output_key: str = "QA"
):
    ...
```

The `run` function executes the main QA pairs generation workflow:
read data paths â†’ **validate DataFrame** â†’ construct prompts â†’ call the model â†’ generate QA pairs captions â†’ write results to output.

## ğŸ§¾ `run` å‚æ•°

| Parameter         | Type              | Default     | Description                                           |
| :---------------- | :---------------- | :---------- | :---------------------------------------------------- |
| `storage`         | `DataFlowStorage` | -           | Dataflow storage object                               |
| `input_mets_key`  | `str`             | `"Meta"`    | **Multimodal Input Field Name**                       |
| `input_clips_key` | `str`             | `"Clips"`   | **Multimodal Input Field Name**                       |
| `output_key`      | `str`             | `"QA"`      | **Model Output Field Name** (the generated QA pairs)  |

-----

## ğŸ§  ç¤ºä¾‹ç”¨æ³•

```python
import os 
import argparse
from dataflow.serving import LocalModelVLMServing_vllm
from dataflow.utils.storage import FileStorage
from dataflow.operators.core_vision import MultiroleVideoQAInitialGenerator, MultiroleVideoQAMultiAgentGenerator, MultiroleVideoQAFinalGenerator

# Step 1: Launch local model service
llm_serving = LocalModelVLMServing_vllm(
            hf_model_name_or_path=model_path,
            hf_cache_dir=hf_cache_dir,
            hf_local_dir=download_dir,
            vllm_tensor_parallel_size=1, 
            vllm_temperature=0.7,
            vllm_top_p=0.9,
            vllm_max_tokens=6000,
        )

# Step 2: Prepare input data
storage = FileStorage(
            first_entry_file_name=first_entry_file,
            cache_path=cache_path,
            file_name_prefix=file_name_prefix,
            cache_type=cache_type,
        )

# Step 3: Initialize and run the operator
initial_QA_generation = MultiroleVideoQAInitialGenerator(llm_serving = self.llm_serving)
multiAgent_QA_generation = MultiroleVideoQAMultiAgentGenerator(llm_serving = self.llm_serving, max_iterations = 3)
final_QA_generation = MultiroleVideoQAFinalGenerator(llm_serving = self.llm_serving)

init_df = initial_QA_generation.run(
            storage = self.storage.step(),
            input_meta_key = self.input_meta_key, 
            input_clips_key = self.input_clips_key, 
            output_key = self.output_key
        )
middle_df = multiAgent_QA_generation.run(
            df = init_df,
            input_meta_key = self.input_meta_key, 
            input_clips_key = self.input_clips_key, 
            output_key = self.output_key
        )
final_QA_generation.run(
            storage = self.storage,
            df = middle_df,
            input_meta_key = self.input_meta_key, 
            input_clips_key = self.input_clips_key, 
            output_key = self.output_key
        )
```

-----

## ğŸ§¾ é»˜è®¤è¾“å‡ºæ ¼å¼

| Field     | Type         | Description                      |
| :-------- | :----------- | :------------------------------- |
| `Meta`    | `str`        | Meta information for video       |
| `Clips`   | `List[Dict]` | Interleaved modality video Clips |
| `QA`      | `List[Dict]` | QA pairs                         |

-----

### ğŸ“¥ ç¤ºä¾‹è¾“å…¥

```jsonl
{"Meta": "Meta Information", "Clips": [{"Audio_Text": "Audio_Text1", "Frames_Images": ["image_path1","image_path2"], "Description": "Description1"}, {"Audio_Text": "Audio_Text2", "Frames_Images": ["image_path3","image_path4"], "Description": "Description2"}]}
```

### ğŸ“¤ ç¤ºä¾‹è¾“å‡º

```jsonl
{"Meta": "Meta Information", "Clips": [{"Audio_Text": "Audio_Text1", "Frames_Images": ["image_path1","image_path2"], "Description": "Description1"}, {"Audio_Text": "Audio_Text2", "Frames_Images": ["image_path3","image_path4"], "Description": "Description2"}], "QA":[{"Label":"label1", "Question": "Question1", "Answer": "Answer1"},{"Label":"label2", "Question": "Question2", "Answer": "Answer2"}]}
```