---
title: ContextVQA Multimodal QA Data Generation Pipeline (API Version) 
icon: mdi:image-text 
createTime: 2026/01/24 16:37:37 
permalink: /en/mm_guide/contextvqa_api_pipeline/
---

## 1. Overview

The **ContextVQA Multimodal QA Data Generation Pipeline (API Version)** is designed to automatically generate **Context-based Visual Question Answering (VQA) data** starting from images. This pipeline utilizes Vision-Language Models (VLM) via API to generate Wikipedia-style articles and QA pairs, which are then parsed into structured data. It is ideal for building knowledge-intensive VQA and multimodal RAG (Retrieval-Augmented Generation) datasets.

We support the following use cases:

* **Knowledge-based VQA Data Synthesis**: Building QA datasets that require external knowledge reasoning.
* **Multimodal RAG Data Construction**: Generating high-quality data for training RAG systems.
* **Visual Reasoning Training**: Generating data where questions refer to an image, but answers must be reasoned from text context.

The pipeline consists of three main stages:

1. **Data Loading**: Reading data files containing image paths.
2. **Context and QA Generation**: Using VLM APIs to generate Wikipedia-style articles and raw QA pairs based on images.
3. **Data Cleaning and Structuring**: Parsing raw text to extract structured `{context, qas}` formats.

---

## 2. Quick Start

### Step 1: Configure API Key

Set your API Key environment variable in your script:

```python
import os
os.environ["DF_API_KEY"] = "your_api_key"

```

### Step 2: Create a New DataFlow Working Directory

```bash
mkdir run_dataflow
cd run_dataflow

```

### Step 3: Initialize DataFlow-MM

```bash
dataflowmm init

```

You will then see:

```bash
api_pipelines/image_contextvqa.py

```

### Step 4: Download Sample Data

```bash
huggingface-cli download --repo-type dataset OpenDCAI/dataflow-demo-image --local-dir data

```

### Step 5: Configure Parameters

Configure the API service and input data paths in `image_contextvqa.py`:

```python
self.vlm_serving = APIVLMServing_openai(
    api_url="https://dashscope.aliyuncs.com/compatible-mode/v1", # Any OpenAI-compatible API platform
    key_name_of_api_key="DF_API_KEY", # API key set in Step 1
    model_name="qwen3-vl-8b-instruct",
    max_workers=10,
    timeout=1800
)

```

```python
parser.add_argument("--images_file", default="data/image_contextvqa/sample_data.json")
parser.add_argument("--cache_path", default="./cache_local")
parser.add_argument("--file_name_prefix", default="context_vqa")
parser.add_argument("--cache_type", default="json")

```

### Step 6: Run with One Command

```bash
python api_pipelines/image_contextvqa.py

```

---

## 3. Data Flow and Pipeline Logic

### 1. **Input Data**

The input data for this process primarily includes the following fields:

* **image**: Image file path (local path or URL).
* **id** (Optional): Unique identifier for the data.
* **conversation** (Optional): Conversation-formatted text used to guide context generation.

Data is managed via `FileStorage`, supporting breakpoint resumption.

**Input Data Example**:

```json
[
    {
        "image": ["./data/image_contextvqa/person.png"],
        "conversation": [
            {
                "from": "human",
                "value": "Write a Wikipedia article related to this image without directly referring to the image. Then write question answer pairs..."
            }
        ]
    }
]

```

### 2. **Core Operator Logic**

The pipeline chains two core operators:

#### A. **PromptedVQAGenerator (Context Generation)**

This operator calls the VLM API to generate raw text based on the prompt template.

**Features:**

* Generates a Wikipedia-style encyclopedia article based on the image.
* Generates QA pairs based on the article.
* **Prompt Constraints**: Questions point to the image but avoid direct object naming; answers must come from the article and not be objects in the image; answers must be concise.

**Operator Execution**:

```python
self.vqa_generator.run(
    storage=self.storage.step(),
    input_conversation_key="conversation",
    input_image_key="image",
    output_answer_key="vqa"
)

```

#### B. **WikiQARefiner (Result Parsing)**

This operator cleans the unstructured text generated by the VLM and converts it into a standard format.

**Features:**

* Cleans Markdown formatting and redundant whitespace.
* Separates article content (Context) and QA pairs (QAs).

**Operator Execution**:

```python
self.refiner.run(
    storage=self.storage.step(),
    input_key="vqa",
    output_key="context_vqa"
)

```

### 3. **Output Data**

The final output contains:

* **image**: Original image path.
* **vqa**: Raw text generated by VLM (intermediate result).
* **context_vqa**: Structured final result containing `context` (article) and `qas` (QA list).

**Output Data Example**:

```json
[
  {
    "image": ["./data/image_contextvqa/person.png"],
    "context_vqa": {
      "context": "**Wikipedia Article:** Nightmare Alley is a 2021 American psychological thriller film...",
      "qas": [
        {
          "question": "What genre does this film belong to?",
          "answer": "Psychological thriller"
        },
        {
          "question": "Who directed this film?",
          "answer": "Guillermo del Toro"
        }
      ]
    }
  }
]

```

---

## 4. Pipeline Example

The following is the complete `ContextVQAPipeline` implementation supporting CLI arguments.

```python
import os
import argparse
from dataflow.utils.storage import FileStorage
from dataflow.serving.api_vlm_serving_openai import APIVLMServing_openai
from dataflow.operators.core_vision import PromptedVQAGenerator
from dataflow.operators.core_vision import WikiQARefiner

# Set API Key environment variable
os.environ["DF_API_KEY"] = "sk-xxxx"

class ContextVQAPipeline:
    """
    Generate batch ContextVQA captions with a single command.
    """

    def __init__(
        self,
        first_entry_file: str = "dataflow/example/image_to_text_pipeline/capsbench_captions.jsonl",
        cache_path: str = "./cache_local_skvqa",
        file_name_prefix: str = "skvqa_cache_step",
        cache_type: str = "jsonl",
    ):
        # ---------- 1. Storage ----------
        self.storage = FileStorage(
            first_entry_file_name=first_entry_file,
            cache_path=cache_path,
            file_name_prefix=file_name_prefix,
            cache_type=cache_type,
        )

        # ---------- 2. Serving ----------
        self.vlm_serving = APIVLMServing_openai(
            api_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            key_name_of_api_key="DF_API_KEY",
            model_name="qwen3-vl-8b-instruct",
            image_io=None,
            send_request_stream=False,
            max_workers=10,
            timeout=1800
        )

        # ---------- 3. Operator ----------
        self.vqa_generator = PromptedVQAGenerator(
            serving=self.vlm_serving,
            system_prompt="You are a helpful assistant."
        )

        self.refiner = WikiQARefiner()

    def forward(self):
        input_image_key = "image"
        output_answer_key = "vqa"
        output_wiki_key = "context_vqa"

        self.vqa_generator.run(
            storage=self.storage.step(),
            input_conversation_key="conversation",
            input_image_key=input_image_key,
            output_answer_key=output_answer_key,
        )

        self.refiner.run(
            storage=self.storage.step(),
            input_key=output_answer_key,
            output_key=output_wiki_key
        )

# ---------------------------- CLI Entry -------------------------------- #
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Batch ContextVQA generation with DataFlow")

    parser.add_argument("--images_file", default="data/image_contextvqa/sample_data.json")
    parser.add_argument("--cache_path", default="./cache_local")
    parser.add_argument("--file_name_prefix", default="context_vqa")
    parser.add_argument("--cache_type", default="json")

    args = parser.parse_args()

    pipe = ContextVQAPipeline(
        first_entry_file=args.images_file,
        cache_path=args.cache_path,
        file_name_prefix=args.file_name_prefix,
        cache_type=args.cache_type,
    )
    pipe.forward()

```