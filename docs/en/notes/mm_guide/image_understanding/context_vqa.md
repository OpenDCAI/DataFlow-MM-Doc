---
title: ContextVQA Multimodal QA Data Generation Pipeline
icon: mdi:image-text
createTime: 2026/01/24 15:37:37
permalink: /en/mm_guide/contextvqa_pipeline/
---

## 1. Overview

The **ContextVQA Multimodal QA Data Generation Pipeline** is designed to automatically generate **Visual Question Answering (VQA) data equipped with external knowledge contexts** starting from images. This pipeline utilizes Vision-Language Models (VLM) to generate Wikipedia-style articles related to the image and corresponding QA pairs, which are then parsed into structured data.



We support the following application scenarios:

* **Knowledge-based VQA Data Synthesis**: Building QA datasets that require external knowledge reasoning.
* **Multimodal RAG Data Construction**: Generating high-quality data for training Retrieval-Augmented Generation (RAG) systems.
* **Visual Reasoning Training**: Generating data where the question points to the image, but the answer must be reasoned from the accompanying text context.

The main stages of the pipeline include:

1.  **Data Loading**: Reading data files containing image paths.
2.  **Context and QA Generation**: Using a VLM to generate a Wikipedia-style article and raw QA pairs based on the image.
3.  **Data Cleaning and Structuring**: Parsing raw text to extract a structured `{context, qas}` format.

---

## 2. Quick Start

### Step 1: Prepare Working Directory

```bash
mkdir run_context_vqa
cd run_context_vqa

```

### Step 2: Prepare Script

Save the code from the "Pipeline Example" section below as `context_vqa_pipeline.py`.

### Step 3: Configure Parameters

The pipeline supports command-line parameter configuration. You can specify the model path and input file directly:

```bash
# Ensure relevant dependencies are installed
pip install open-dataflow vllm

```

### Step 4: Run with One Command

```bash
python context_vqa_pipeline.py \
  --model_path "Qwen/Qwen2.5-VL-3B-Instruct" \
  --images_file "path/to/your/images.jsonl" \
  --cache_path "./cache_local"

```

---

## 3. Data Flow and Pipeline Logic

### 1. **Input Data**

The input data for this process primarily contains the following fields:

* **image**: Path to the image file (local path or URL).
* **id** (optional): Unique identifier for the data.

Data is managed via `FileStorage`, which supports breakpoint resumption (checkpointing).

**Input Data Example**:

```jsonl
{"id": 1, "image": "./images/landmark.jpg"}
{"id": 2, "image": "./images/animal.jpg"}

```

Example images can be found at `https://huggingface.co/datasets/OpenDCAI/dataflow-demo-image/tree/main/capsbench_images`. Additionally, we have synthesized 200k high-quality context VQA data records for the community to experience at `https://huggingface.co/datasets/OpenDCAI/dataflow-mm-context_vqa`.

### 2. **Core Operator Logic**

The pipeline completes its task by concatenating two core operators:

#### A. **FixPromptedVQAGenerator (Context Generation)**

This operator uses the VLM model to generate raw text according to a preset prompt template.

**Functions:**

* Generates a Wikipedia-style science article based on the image.
* Generates QA pairs based on the article.
* **Prompt Constraints**: The question points to the image but avoids direct mention of object names; answers must come from the article content and not be objects in the image; answers should be concise.

**Model Serving Configuration**:

```python
self.serving = LocalModelVLMServing_vllm(
    hf_model_name_or_path=model_path,
    hf_cache_dir=hf_cache_dir,
    vllm_tensor_parallel_size=1,
    vllm_temperature=0.7,  # Maintain a level of creativity
    vllm_top_p=0.9,
    vllm_max_tokens=512,
)

```

**Operator Execution**:

```python
self.vqa_generator.run(
    storage=self.storage.step(),
    input_image_key="image",
    output_answer_key="vqa" # Outputs the raw generated text
)

```

#### B. **WikiQARefiner (Result Refinement)**

This operator is responsible for cleaning the unstructured text generated by the VLM and converting it into a standard format.

**Functions:**

* Cleans Markdown formatting and redundant white space.
* Separates article content (Context) from QA pairs (QAs).

**Operator Execution**:

```python
self.refiner.run(
    storage=self.storage.step(),
    input_key="vqa",          # Inputs raw text from the previous step
    output_key="context_vqa"  # Outputs final structured data
)

```

### 3. **Output Data**

Ultimately, the output generated by the pipeline will include:

* **image**: Original image path.
* **vqa**: Raw text generated by the VLM (intermediate result).
* **context_vqa**: Final structured result containing `context` (article) and `qas` (QA list).

**Output Data Example**:

```json
{
    "id": 1,
    "image": "./images/landmark.jpg",
    "context_vqa": {
        "context": "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France...",
        "qas": [
            {
                "question": "In which city is this structure located?",
                "answer": "Paris"
            },
            {
                "question": "What material is the tower primarily constructed from?",
                "answer": "wrought-iron"
            }
        ]
    }
}

```

---

## 4. Pipeline Example

Below is the complete implementation of `ContextVQAPipeline`, supporting command-line arguments.

```python
import argparse
from dataflow.utils.storage import FileStorage
from dataflow.serving.local_model_vlm_serving import LocalModelVLMServing_vllm
from dataflow.operators.core_vision import FixPromptedVQAGenerator
from dataflow.operators.core_vision import WikiQARefiner

class ContextVQAPipeline:
    """
    Complete batch ContextVQA Caption generation for images with a single command.
    """

    def __init__(
        self,
        model_path: str,
        *,
        hf_cache_dir: str | None = None,
        download_dir: str = "./ckpt",
        device: str = "cuda",
        first_entry_file: str = "dataflow/example/image_to_text_pipeline/capsbench_captions.jsonl",
        cache_path: str = "./cache_local_skvqa",
        file_name_prefix: str = "skvqa_cache_step",
        cache_type: str = "jsonl",
    ):
        # ---------- 1. Storage ----------
        self.storage = FileStorage(
            first_entry_file_name=first_entry_file,
            cache_path=cache_path,
            file_name_prefix=file_name_prefix,
            cache_type=cache_type,
        )

        # ---------- 2. Serving ----------
        self.serving = LocalModelVLMServing_vllm(
            hf_model_name_or_path=model_path,
            hf_cache_dir=hf_cache_dir,
            hf_local_dir=download_dir,
            vllm_tensor_parallel_size=1,
            vllm_temperature=0.7,
            vllm_top_p=0.9,
            vllm_max_tokens=512,
        )

        # ---------- 3. Operator ----------
        # Generate Wiki-style articles and QA using a specific Prompt
        self.vqa_generator = FixPromptedVQAGenerator(
            serving=self.serving,
            system_prompt="You are a helpful assistant.",
            user_prompt= """
            Write a Wikipedia article related to this image without directly referring to the image. Then write question answer pairs. The question answer pairs should satisfy the following criteria.
            1: The question should refer to the image.
            2: The question should avoid mentioning the name of the object in the image.
            3: The question should be answered by reasoning over the Wikipedia article.
            4: The question should sound natural and concise.
            5: The answer should be extracted from the Wikipedia article.
            6: The answer should not be any objects in the image.
            7: The answer should be a single word or phrase and list all correct answers separated by commas.
            8: The answer should not contain 'and', 'or', rather you can split them into multiple answers.
            """
        )

        # Result cleaning and structuring
        self.refiner = WikiQARefiner()

    # ------------------------------------------------------------------ #
    def forward(self):
        input_image_key = "image"
        output_answer_key = "vqa"
        output_wiki_key = "context_vqa"

        # Step 1: Generate raw text
        self.vqa_generator.run(
            storage=self.storage.step(),
            input_image_key=input_image_key,
            output_answer_key=output_answer_key
        )

        # Step 2: Parse into structured data
        self.refiner.run(
            storage=self.storage.step(),
            input_key=output_answer_key,
            output_key=output_wiki_key
        )

# ---------------------------- CLI Entry -------------------------------- #
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Batch SKVQA caption generation with DataFlow")

    parser.add_argument("--model_path", default="Qwen/Qwen2.5-VL-3B-Instruct")
    parser.add_argument("--hf_cache_dir", default="~/.cache/huggingface")
    parser.add_argument("--download_dir", default="./ckpt")
    parser.add_argument("--device", choices=["cuda", "cpu", "mps"], default="cuda")

    parser.add_argument("--images_file", default="dataflow/example/image_to_text_pipeline/capsbench_captions.jsonl")
    parser.add_argument("--cache_path", default="./cache_local")
    parser.add_argument("--file_name_prefix", default="context_vqa")
    parser.add_argument("--cache_type", default="jsonl")

    args = parser.parse_args()

    pipe = ContextVQAPipeline(
        model_path=args.model_path,
        hf_cache_dir=args.hf_cache_dir,
        download_dir=args.download_dir,
        device=args.device,
        first_entry_file=args.images_file,
        cache_path=args.cache_path,
        file_name_prefix=args.file_name_prefix,
        cache_type=args.cache_type,
    )
    pipe.forward()

```